

# Training datasets

| **Dataset Type**     | **Purpose**                                       | **Description**                                                                                          | **Typical Proportion**                               |
| -------------------- | ------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| **Training Set**     | Learn model parameters                            | The main dataset used to train the model; the model "sees" this data and adjusts its internal weights    | 60–80% of total data                                 |
| **Validation Set**   | Tune hyperparameters, prevent overfitting         | Used during training to evaluate model performance and fine-tune hyperparameters; not used to train      | 10–20% of total data                                 |
| **Test Set**         | Final evaluation of model performance             | Used only after training and validation to assess how the model performs on completely unseen data       | 10–20% of total data                                 |
| **Holdout Set**      | Sometimes used as a separate test set             | Extra unseen data kept aside, often in large projects, to verify model generalization at the end         | Variable, if used                                    |
| **Cross-Validation** | Evaluate model stability and performance robustly | Splits data multiple times (e.g., k-fold) into training and validation folds to get averaged performance | Not a fixed proportion; folds vary (e.g., k=5 or 10) |

If you want, I can also explain the pros and cons or variations like stratified splits or time-series splits! Would you like that?


Sure! Here’s a simple **workflow diagram** of a typical ML/AI system, showing where the datasets fit in during the process:

---

### ML/AI System Workflow with Dataset Placement

```plaintext
Raw Data
   │
   ▼
Data Preprocessing (cleaning, feature engineering)
   │
   ▼
Data Splitting ──────────────────────────────┐
   │                                         │
   ▼                                         │
Training Set ------------------> Model Training (learn parameters)
   │                                         │
   ▼                                         │
Validation Set ---------------> Model Tuning (hyperparameters, early stopping)
   │                                         │
   ▼                                         │
Model Evaluation (on validation set)         │
   │                                         │
   ▼                                         │
If performance is good? ──No──> Go back to model training/tuning
   │
  Yes
   │
   ▼
Test Set ---------------------> Final Model Evaluation (unseen data)
   │
   ▼
Deployment (model used in real world)
```

---

### Explanation:

1. **Raw Data:** The original dataset collected from various sources.
2. **Data Preprocessing:** Cleaning missing values, encoding categorical variables, scaling, etc.
3. **Data Splitting:** Data is split into Training, Validation, and Test sets.
4. **Training Set:** Used to fit the model.
5. **Validation Set:** Used to tune hyperparameters and prevent overfitting by monitoring performance during training.
6. **Test Set:** Used only once at the end to assess model generalization.
7. **Deployment:** Once tested, the model is deployed for real-world use.

---

Would you like me to create this as a visual diagram or a more detailed step-by-step workflow?
