{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-11T18:14:26.671464Z",
     "start_time": "2025-11-11T18:14:26.668883Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def sigmoid(weights: torch.tensor):\n",
    "    return 1 / (1 + torch.exp(-weights))\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:43:43.550593Z",
     "start_time": "2025-11-11T18:43:43.527479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearModel_V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=3,out_features=10)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=10,out_features=20)\n",
    "        self.linear_layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return sigmoid(self.linear_layer_3(self.linear_layer_2(self.linear_layer_1(x))))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "data_X = torch.rand(100,3)\n",
    "data_y = torch.randint(0,2,(100,1))"
   ],
   "id": "979953f219325411",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:43:42.198705Z",
     "start_time": "2025-11-11T18:43:42.194146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = torch.tensor(data_X[:80], dtype=torch.float32)\n",
    "y_train = torch.tensor(data_y[:80], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(data_X[80:], dtype=torch.float32)\n",
    "y_test = torch.tensor(data_y[80:], dtype=torch.float32)"
   ],
   "id": "2af934ef6d4b498e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/ktj6jq715z1dch5hv4rsd1x80000gn/T/ipykernel_59270/2485558411.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(data_X[:80], dtype=torch.float32)\n",
      "/var/folders/nd/ktj6jq715z1dch5hv4rsd1x80000gn/T/ipykernel_59270/2485558411.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(data_y[:80], dtype=torch.float32)\n",
      "/var/folders/nd/ktj6jq715z1dch5hv4rsd1x80000gn/T/ipykernel_59270/2485558411.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(data_X[80:], dtype=torch.float32)\n",
      "/var/folders/nd/ktj6jq715z1dch5hv4rsd1x80000gn/T/ipykernel_59270/2485558411.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(data_y[80:], dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:49:27.452269Z",
     "start_time": "2025-11-11T18:49:27.389899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_0 = LinearModel_V1()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.01)\n",
    "\n",
    "epochs = 80\n",
    "\n",
    "for x in range(epochs):\n",
    "\n",
    "    y_pred = model_0(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if x % 2 == 0:\n",
    "        print(f'Epoch {x}, Loss: {loss.item()}')"
   ],
   "id": "1d1647bd27964ef1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7162129282951355\n",
      "Epoch 2, Loss: 0.71612948179245\n",
      "Epoch 4, Loss: 0.7160462141036987\n",
      "Epoch 6, Loss: 0.7159631848335266\n",
      "Epoch 8, Loss: 0.715880274772644\n",
      "Epoch 10, Loss: 0.7157977223396301\n",
      "Epoch 12, Loss: 0.7157152891159058\n",
      "Epoch 14, Loss: 0.7156330943107605\n",
      "Epoch 16, Loss: 0.7155510783195496\n",
      "Epoch 18, Loss: 0.7154693603515625\n",
      "Epoch 20, Loss: 0.7153878211975098\n",
      "Epoch 22, Loss: 0.7153065204620361\n",
      "Epoch 24, Loss: 0.7152253985404968\n",
      "Epoch 26, Loss: 0.7151445150375366\n",
      "Epoch 28, Loss: 0.7150638699531555\n",
      "Epoch 30, Loss: 0.7149834036827087\n",
      "Epoch 32, Loss: 0.7149031758308411\n",
      "Epoch 34, Loss: 0.7148231267929077\n",
      "Epoch 36, Loss: 0.7147433757781982\n",
      "Epoch 38, Loss: 0.7146638035774231\n",
      "Epoch 40, Loss: 0.7145844101905823\n",
      "Epoch 42, Loss: 0.7145053148269653\n",
      "Epoch 44, Loss: 0.7144263386726379\n",
      "Epoch 46, Loss: 0.7143476009368896\n",
      "Epoch 48, Loss: 0.7142691612243652\n",
      "Epoch 50, Loss: 0.7141908407211304\n",
      "Epoch 52, Loss: 0.7141128182411194\n",
      "Epoch 54, Loss: 0.7140349745750427\n",
      "Epoch 56, Loss: 0.7139573693275452\n",
      "Epoch 58, Loss: 0.7138799428939819\n",
      "Epoch 60, Loss: 0.713802695274353\n",
      "Epoch 62, Loss: 0.713725745677948\n",
      "Epoch 64, Loss: 0.7136489748954773\n",
      "Epoch 66, Loss: 0.7135725021362305\n",
      "Epoch 68, Loss: 0.7134960889816284\n",
      "Epoch 70, Loss: 0.713420033454895\n",
      "Epoch 72, Loss: 0.7133442163467407\n",
      "Epoch 74, Loss: 0.713268518447876\n",
      "Epoch 76, Loss: 0.7131930589675903\n",
      "Epoch 78, Loss: 0.713117778301239\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:49:38.154475Z",
     "start_time": "2025-11-11T18:49:38.128869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for x in X_test:\n",
    "    print(model_0(x))"
   ],
   "id": "3d7a4d1686f44bb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5164], grad_fn=<MulBackward0>)\n",
      "tensor([0.5123], grad_fn=<MulBackward0>)\n",
      "tensor([0.5148], grad_fn=<MulBackward0>)\n",
      "tensor([0.5065], grad_fn=<MulBackward0>)\n",
      "tensor([0.5181], grad_fn=<MulBackward0>)\n",
      "tensor([0.5090], grad_fn=<MulBackward0>)\n",
      "tensor([0.5220], grad_fn=<MulBackward0>)\n",
      "tensor([0.5100], grad_fn=<MulBackward0>)\n",
      "tensor([0.5070], grad_fn=<MulBackward0>)\n",
      "tensor([0.5077], grad_fn=<MulBackward0>)\n",
      "tensor([0.5314], grad_fn=<MulBackward0>)\n",
      "tensor([0.5035], grad_fn=<MulBackward0>)\n",
      "tensor([0.5285], grad_fn=<MulBackward0>)\n",
      "tensor([0.5164], grad_fn=<MulBackward0>)\n",
      "tensor([0.5162], grad_fn=<MulBackward0>)\n",
      "tensor([0.5382], grad_fn=<MulBackward0>)\n",
      "tensor([0.5129], grad_fn=<MulBackward0>)\n",
      "tensor([0.5129], grad_fn=<MulBackward0>)\n",
      "tensor([0.5108], grad_fn=<MulBackward0>)\n",
      "tensor([0.5162], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The Core Difference: What You’re Predicting\n",
    "\n",
    "| Task                  | Target (y)        | Goal                  | Example                      |\n",
    "| --------------------- | ----------------- | --------------------- | ---------------------------- |\n",
    "| **Linear Regression** | Continuous value  | Predict *how much*    | Predict house price          |\n",
    "| **Classification**    | Categorical label | Predict *which class* | Is this email spam? (Yes/No) |\n",
    "\n",
    "So the difference starts with **what the target variable represents**.\n",
    "Everything else — activation, loss function, and interpretation — follows from that.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture Differences\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "* **Output:** Usually **1 neuron**, no activation (raw value).\n",
    "* **Example:**\n",
    "\n",
    "  ```python\n",
    "  output = model(X)  # shape [N, 1]\n",
    "  ```\n",
    "* **Why no activation?**\n",
    "  Because regression outputs are continuous — you want to allow any real number.\n",
    "\n",
    "---\n",
    "\n",
    "### **Classification**\n",
    "\n",
    "* **Output:** Depends on the number of classes.\n",
    "\n",
    "  * Binary: **1 output neuron** with a **sigmoid activation**\n",
    "  * Multi-class: **n output neurons** with a **softmax activation**\n",
    "\n",
    "#### **Binary classification**\n",
    "\n",
    "```python\n",
    "output = torch.sigmoid(model(X))\n",
    "```\n",
    "\n",
    "→ Produces values in [0,1], interpretable as “probability of class 1”.\n",
    "\n",
    "#### **Multi-class classification**\n",
    "\n",
    "```python\n",
    "output = torch.softmax(model(X), dim=1)\n",
    "```\n",
    "\n",
    "→ Produces a probability distribution across classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "| Task                           | Typical Loss                                                  |\n",
    "| ------------------------------ | ------------------------------------------------------------- |\n",
    "| **Linear Regression**          | Mean Squared Error (`nn.MSELoss`)                             |\n",
    "| **Binary Classification**      | Binary Cross-Entropy (`nn.BCELoss` or `nn.BCEWithLogitsLoss`) |\n",
    "| **Multi-class Classification** | Cross-Entropy (`nn.CrossEntropyLoss`)                         |\n",
    "\n",
    "---\n",
    "\n",
    "## Activation + Loss Connection\n",
    "\n",
    "| Output Activation | Corresponding Loss               | Comment                    |\n",
    "| ----------------- | -------------------------------- | -------------------------- |\n",
    "| **None**          | `MSELoss`                        | regression                 |\n",
    "| **Sigmoid**       | `BCELoss` or `BCEWithLogitsLoss` | binary classification      |\n",
    "| **Softmax**       | `CrossEntropyLoss`               | multi-class classification |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary in One Line\n",
    "\n",
    "| Task                       | Output Layer | Activation | Loss                        |\n",
    "| -------------------------- | ------------ | ---------- | --------------------------- |\n",
    "| Linear Regression          | 1 neuron     | None       | MSELoss                     |\n",
    "| Binary Classification      | 1 neuron     | Sigmoid    | BCELoss / BCEWithLogitsLoss |\n",
    "| Multi-class Classification | n neurons    | Softmax    | CrossEntropyLoss            |\n",
    "\n",
    "---\n",
    "\n",
    " **So yes — you’re correct:**\n",
    "The *main architectural difference* between regression and classification networks is **the output activation (and the corresponding loss function)**.\n",
    "\n",
    "But conceptually, they also differ in **what they predict and how the output is interpreted** — continuous vs probability.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can show a **side-by-side code example** of the same dataset trained once as a regression and once as a classification network — you’ll see the differences in the last layer and loss directly.\n",
    "\n",
    "Would you like that?\n"
   ],
   "id": "a2eb424efbb12260"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
