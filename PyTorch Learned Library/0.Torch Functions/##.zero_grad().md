

# ðŸ”¹ `optimizer.zero_grad()`

## ðŸ”¸ What it does

* Resets the **gradients** of all model parameters to **zero**.
* In PyTorch, gradients **accumulate** by default.

  * Every call to `loss.backward()` **adds** (sums) new gradients to `.grad` attributes of parameters.
  * If you donâ€™t clear them, gradients from previous batches mix with new ones â†’ wrong updates.

So, `optimizer.zero_grad()` ensures **only the current batchâ€™s gradients** are used when updating weights.

---

## ðŸ”¸ Why do gradients accumulate in PyTorch?

* Gradient accumulation allows:

  * **Gradient accumulation over micro-batches** (simulate large batch sizes by summing multiple smaller batches before calling `optimizer.step()`).
  * More flexibility for custom optimization loops.
* But in standard training, we want fresh gradients each iteration â†’ we reset them.

---

## ðŸ”¸ Where it fits in the training loop

**Typical cycle:**

1. **Forward pass** â†’ compute predictions.
2. **Loss computation**.
3. **Zero gradients** (`optimizer.zero_grad()`).

   * Clear previous batchâ€™s gradients.
4. **Backward pass** (`loss.backward()`).

   * Compute new gradients w\.r.t parameters.
5. **Update step** (`optimizer.step()`).

   * Use the gradients to update model parameters.

---

## ðŸ”¸ Example

```python
for epoch in range(5):
    for inputs, targets in dataloader:
        
        # 1. Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # 2. Zero old gradients
        optimizer.zero_grad()

        # 3. Backward pass
        loss.backward()

        # 4. Update weights
        optimizer.step()
```

---

## ðŸ”¸ Common mistakes

1. **Forgetting `zero_grad()`** â†’

   * Gradients from multiple batches accumulate.
   * Leads to unstable training (loss doesnâ€™t decrease as expected).

2. **Calling it in the wrong place** â†’

   * If placed **after `loss.backward()`**, the freshly computed gradients get cleared before update.
   * Must always be **before `loss.backward()`**.

---

## ðŸ”¸ Variants

* `optimizer.zero_grad(set_to_none=True)`

  * Instead of setting `.grad = 0`, it sets them to `None`.
  * Saves memory and sometimes speed.
  * Works identically for training but some edge cases differ (e.g., checking if `.grad` exists).

---

âœ… **Summary:**
`optimizer.zero_grad()` is the **reset button for gradients** in a training loop. Without it, gradients accumulate across iterations, leading to incorrect parameter updates. It must be called **before** `loss.backward()` in every training iteration.

