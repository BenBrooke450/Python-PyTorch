

# ðŸ”¹ `optimizer.step()`

## ðŸ”¸ What it does

* Applies a **parameter update** using the gradients that were just computed by `loss.backward()`.

* For each parameter $\theta$ in the model:

  $$
  \theta \leftarrow \theta - \eta \cdot g
  $$

  where:

  * $\eta$ = learning rate
  * $g = \frac{\partial L}{\partial \theta}$ = gradient stored in `param.grad`

* If the optimizer has extras like **momentum, weight decay, or Adamâ€™s adaptive learning rates**, `.step()` handles those details internally.

---

## ðŸ”¸ Why it matters

* Itâ€™s the **actual step where learning happens**.
* Until you call `.step()`, the modelâ€™s parameters are unchanged â€” gradients are just sitting in `.grad`.

---

## ðŸ”¸ Where it fits in the training loop

1. **Forward pass** â†’ predictions.
2. **Loss calculation**.
3. **Zero gradients** â†’ `optimizer.zero_grad()`.
4. **Backward pass** â†’ `loss.backward()` (gradients stored in `.grad`).
5. **Parameter update** â†’ `optimizer.step()` (weights change).

---

## ðŸ”¸ Example

```python
for inputs, targets in dataloader:
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    optimizer.zero_grad()   # reset old gradients
    loss.backward()         # compute new gradients
    optimizer.step()        # update parameters
```

---

## ðŸ”¸ Example with math (SGD)

Suppose:

* Weight $w = 2.0$, gradient $g = 0.3$, learning rate $\eta = 0.1$.

After `optimizer.step()`:

$$
w \leftarrow 2.0 - 0.1 \times 0.3 = 1.97
$$

So the model "learns" by shifting weights slightly to reduce the loss.

---

## ðŸ”¸ Variants

* For **SGD**: simple weight update, possibly with **momentum** or **Nesterov**.
* For **Adam**: `.step()` updates parameters using running averages of gradients and squared gradients.
* For **RMSprop, Adagrad, etc.**: each has its own formula, but `.step()` is the place where those updates are executed.

---

## ðŸ”¸ Common mistakes

1. Forgetting to call `optimizer.step()` â†’ model never updates (loss stays flat).
2. Calling `.step()` before `loss.backward()` â†’ parameters update with **stale gradients** from last iteration.
3. Mixing multiple optimizers without care â†’ some parameters wonâ€™t update if not included in the correct optimizer.

---

âœ… **Summary:**
`optimizer.step()` is the command that tells PyTorch to **update all model parameters using their current gradients** according to the optimizerâ€™s algorithm (SGD, Adam, etc.). It must always come **after `loss.backward()`** in the training loop, since thatâ€™s when gradients are ready.

