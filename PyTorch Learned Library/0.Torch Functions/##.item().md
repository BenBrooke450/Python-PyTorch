

## **`tensor.item()` in PyTorch**

### **What it does:**

* `.item()` **extracts a standard Python number** (int or float) from a **tensor containing a single value**.
* It only works if the tensor has **exactly one element**.
* Useful when you want to **use the value in Python code**, logging, or plotting, rather than keeping it as a tensor.

---

### **Why use it:**

1. PyTorch tensors can hold multiple elements and support GPU operations.
2. But sometimes you need a plain Python scalar (for printing, math, or external libraries).
3. `.item()` converts:

```python
tensor([3.5])  →  3.5  (Python float)
tensor([7])    →  7    (Python int)
```

---

### **Basic Example**

```python
import torch

x = torch.tensor([5])      # single-element tensor
y = x.item()                # extract Python int

print(y)                    # 5
print(type(y))              # <class 'int'>
```

---

### **With float tensor**

```python
x = torch.tensor([3.14])
y = x.item()

print(y)                    # 3.14
print(type(y))              # <class 'float'>
```

---

### **Example in a training loop**

```python
loss = torch.tensor([2.718])
print("Loss:", loss.item())  # Loss: 2.718
```

* Useful because logging frameworks or `print()` expect a Python number, not a tensor.

---

### **Important Note**

* `.item()` **only works on single-element tensors**:

```python
x = torch.tensor([1, 2, 3])
x.item()   # ❌ ERROR: "only one element tensors can be converted to Python scalars"
```

* If your tensor has multiple elements, use `.tolist()` instead:

```python
x = torch.tensor([1, 2, 3])
x.tolist()   # [1, 2, 3]  → Python list
```

---

 **Summary Table**

| Tensor           | `.item()` Result | Python Type |
| ---------------- | ---------------- | ----------- |
| `tensor([5])`    | `5`              | `int`       |
| `tensor([3.14])` | `3.14`           | `float`     |
| `tensor([1,2])`  | ❌ Error          | -           |

---

<br><br><br><br><br><br><br><br><br>





---

## **Where Loss Values Go**

By default, **PyTorch does not automatically store losses** after each epoch.
If you want to visualize the loss (for example, as a line chart), you must **manually store it** — usually in a Python list.

---

## ** How to Store Loss Values**

Here’s how you can modify your training loop:

```python
losses = []  # create an empty list to store loss values

for epoch in range(epochs):
    y_pred = model_iris_one(X_train)
    loss = loss_fn(y_pred, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Store loss for visualization
    losses.append(loss.item())

    if epoch % 2 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')
```

Now you have a list called `losses` that contains the loss after each epoch.

