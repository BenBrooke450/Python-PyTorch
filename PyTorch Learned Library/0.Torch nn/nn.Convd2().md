
# **What is `nn.Conv2d`?**

`nn.Conv2d` is the PyTorch layer used to scan **2D data** (images or image-like structures) using **learnable filters (kernels)**.

It is the *core* building block for CNNs.

---

# **1. Input Shape Required**

`nn.Conv2d` expects input in the following shape:

```
(batch_size, in_channels, height, width)
```

### Examples:

* RGB image 256Ã—256 â†’ shape: `(1, 3, 256, 256)`
* Grayscale image â†’ `(1, 1, H, W)`
* Mini-batch of 32 RGB images â†’ `(32, 3, H, W)`

---

# **2. The Parameters of `nn.Conv2d`**

Definition:

```python
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1)
```

Letâ€™s break each part down deeply:

---

## âœ” **a) `in_channels` â€” number of channels in input**

Examples:

* RGB image â†’ 3 channels
* Grayscale â†’ 1 channel
* Feature maps from a previous layer â†’ could be 16, 32, 64, â€¦

---

## âœ” **b) `out_channels` â€” number of filters**

This is how many feature detectors the layer learns.

Examples:

* 16 means the layer learns 16 filters â†’ output has 16 channels.
* 32 means 32 filters â†’ 32 channels output.

These filters detect different visual features:

* Edges
* Corners
* Textures
* Patterns
* Shapes

---

## âœ” **c) `kernel_size` â€” filter size**

Common kernel sizes:

* 3Ã—3 (most common in modern CNNs)
* 5Ã—5
* 7Ã—7
* or even (height, width) tuples like `(3,5)`

### Example:

`kernel_size=3` means a 3Ã—3 sliding window.

---

## âœ” **d) `stride` â€” how far the filter moves each step**

* `stride=1`: move by 1 pixel â†’ high detail
* `stride=2`: move by 2 pixels â†’ aggressively shrink image (downsampling)

---

## âœ” **e) `padding` â€” add zeros around the border**

Used to control output size.

* `padding=0` â†’ output becomes smaller than input.
* `padding=1` with 3Ã—3 filters keeps the output size the same.

**Why padding?**
A 3Ã—3 filter reduces dimensions.
Padding prevents image shrinking every layer.

---

## âœ” **f) `dilation` â€” spacing inside the kernel**

Used rarely (for segmentation).
Expands receptive field without increasing kernel size.

---

# **3. How It Actually Calculates Output (Mechanics)**

The convolution operation does:

1. Place a kernel (example: 3Ã—3) on top-left of the image.
2. Multiply kernel values by the overlapping pixel values.
3. Sum them â†’ this becomes ONE output pixel.
4. Slide the kernel over the whole image (controlled by stride).
5. Repeat for each kernel â†’ producing multiple output channels.

---

# **4. Output Shape Formula**

For each spatial dimension:

```
Out = âŒŠ(W - K + 2P) / SâŒ‹ + 1
```

Where:

| Symbol | Meaning            |
| ------ | ------------------ |
| W      | input width/height |
| K      | kernel size        |
| P      | padding            |
| S      | stride             |

### Full output shape:

```
(batch_size, out_channels, out_height, out_width)
```

---

# Example Calculation

Input: `(1, 3, 32, 32)`
Layer:

```python
nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
```

Output height:

```
(32 - 3 + 2*1) / 1 + 1 = 32
```

Output shape = `(1, 16, 32, 32)`

Same size because padding=1.

---

# **5. What Conv2d Learns**

Each filter learns to detect visual patterns:

* Vertical edges
* Horizontal edges
* Diagonals
* Curves
* Blobs
* Shapes
* Textures

Deeper layers detect:

* Eyes, noses, wheels, paws
* Organs in medical scans
* Tumor edges

Eventually:

* High-level concepts (normal vs abnormal)

---

# ğŸ¯ **6. Common Patterns in CNN Architecture**

### Typical convolution block:

```python
nn.Conv2d(...)
nn.ReLU()
nn.MaxPool2d(2)
```

### More modern:

```python
nn.Conv2d(...)
nn.BatchNorm2d(...)
nn.ReLU()
```

---

# ğŸ§ª **7. Small Practical Example**

```python
import torch.nn as nn

class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc = nn.Linear(32 * 8 * 8, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))   # -> (B,16,32,32)
        x = F.max_pool2d(x, 2)      # -> (B,16,16,16)
        x = F.relu(self.conv2(x))   # -> (B,32,16,16)
        x = F.max_pool2d(x, 2)      # -> (B,32,8,8)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
```

---

# ğŸ Final Summary (Very Clear)

| Concept      | Meaning                                 |
| ------------ | --------------------------------------- |
| Conv2d       | Learns filters to scan 2D data (images) |
| in_channels  | Channels entering (RGB=3)               |
| out_channels | Number of filters learned               |
| kernel_size  | Filter size (3Ã—3 typical)               |
| stride       | Movement step                           |
| padding      | Keeps size same or shrinks              |
| output shape | (B, out_channels, H_out, W_out)         |

