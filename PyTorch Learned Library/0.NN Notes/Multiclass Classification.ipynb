{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## **Your current code**\n",
    "\n",
    "```python\n",
    "class DeepLearningNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # <-- small fix: need parentheses here\n",
    "\n",
    "        self.linear_one = nn.Linear(in_features=4, out_features=30)\n",
    "        self.linear_two = nn.Linear(in_features=30, out_features=10)\n",
    "        self.linear_three = nn.Linear(in_features=10, out_features=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.linear_three(self.linear_two(self.linear_one(x))), dim=1)\n",
    "```\n",
    "\n",
    "### **Notes on your current implementation**\n",
    "\n",
    "1. `super().__init__` → **missing parentheses**. Should be `super().__init__()`.\n",
    "2. `torch.softmax` in `forward` is **valid**, but there are important considerations:\n",
    "\n",
    "   * If you plan to use **`nn.CrossEntropyLoss()`**, **do NOT apply softmax in the forward pass**.\n",
    "   * `nn.CrossEntropyLoss()` internally applies `log_softmax` + negative log likelihood, so applying softmax manually will **give wrong results**.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Recommended way for classification**\n",
    "\n",
    "### **Option A: Do NOT apply softmax in the forward pass**\n",
    "\n",
    "```python\n",
    "class DeepLearningNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_one = nn.Linear(4, 30)\n",
    "        self.linear_two = nn.Linear(30, 10)\n",
    "        self.linear_three = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_one(x)\n",
    "        x = torch.relu(x)  # usually apply activation after hidden layers\n",
    "        x = self.linear_two(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_three(x)  # no softmax here\n",
    "        return x\n",
    "```\n",
    "\n",
    "* Then define your **loss function** like this:\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "* PyTorch’s `CrossEntropyLoss` expects **raw logits**, not probabilities.\n",
    "* During inference, you can apply `softmax` to convert logits to probabilities:\n",
    "\n",
    "```python\n",
    "probs = torch.softmax(model(X), dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Option B: Apply softmax manually (not recommended for training)**\n",
    "\n",
    "```python\n",
    "class DeepLearningNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_one = nn.Linear(4, 30)\n",
    "        self.linear_two = nn.Linear(30, 10)\n",
    "        self.linear_three = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear_one(x))\n",
    "        x = torch.relu(self.linear_two(x))\n",
    "        x = torch.softmax(self.linear_three(x), dim=1)\n",
    "        return x\n",
    "```\n",
    "\n",
    "* Here the network **returns probabilities directly**, but you must use `nn.NLLLoss()` with `log` applied **before** the loss, which is cumbersome.\n",
    "* That’s why **Option A is preferred** for classification.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Adding hidden activations properly**\n",
    "\n",
    "* Usually, we apply **non-linearities** (like `ReLU`) between layers.\n",
    "* Without them, your network is just a **linear combination** and cannot learn complex relationships.\n",
    "\n",
    "```python\n",
    "class DeepLearningNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "* During training: `loss = nn.CrossEntropyLoss()(logits, targets)`\n",
    "* During inference: `preds = torch.argmax(logits, dim=1)` or `probs = torch.softmax(logits, dim=1)`\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Other ways to structure forward pass**\n",
    "\n",
    "#### **Method 1: Using `nn.Sequential`**\n",
    "\n",
    "```python\n",
    "class DeepLearningNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(4, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "```\n",
    "\n",
    "* Very concise, especially if you have a **feedforward network** with no branching.\n",
    "\n",
    "#### **Method 2: Using softmax only during inference**\n",
    "\n",
    "```python\n",
    "logits = model(X)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "```\n",
    "\n",
    "* Keeps training stable and correct.\n",
    "\n",
    "#### **Method 3: Custom activation functions**\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = torch.tanh(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    logits = self.fc3(x)\n",
    "    return logits\n",
    "```\n",
    "\n",
    "* You can mix **ReLU, Tanh, Sigmoid** for experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. **Do NOT apply softmax in forward pass if using `CrossEntropyLoss`**.\n",
    "2. Hidden layers usually **need non-linearities** (ReLU, Tanh, etc.).\n",
    "3. `nn.Sequential` is a convenient shorthand for feedforward networks.\n",
    "4. Use `torch.softmax` only **during inference**, if you want probabilities.\n",
    "5. Always fix `super().__init__()` — it’s essential.\n",
    "\n"
   ],
   "id": "4fea9913ee9003c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The Core Difference: What You’re Predicting\n",
    "\n",
    "| Task                  | Target (y)        | Goal                  | Example                      |\n",
    "| --------------------- | ----------------- | --------------------- | ---------------------------- |\n",
    "| **Linear Regression** | Continuous value  | Predict *how much*    | Predict house price          |\n",
    "| **Classification**    | Categorical label | Predict *which class* | Is this email spam? (Yes/No) |\n",
    "\n",
    "So the difference starts with **what the target variable represents**.\n",
    "Everything else — activation, loss function, and interpretation — follows from that.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture Differences\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "* **Output:** Usually **1 neuron**, no activation (raw value).\n",
    "* **Example:**\n",
    "\n",
    "  ```python\n",
    "  output = model(X)  # shape [N, 1]\n",
    "  ```\n",
    "* **Why no activation?**\n",
    "  Because regression outputs are continuous — you want to allow any real number.\n",
    "\n",
    "---\n",
    "\n",
    "### **Classification**\n",
    "\n",
    "* **Output:** Depends on the number of classes.\n",
    "\n",
    "  * Binary: **1 output neuron** with a **sigmoid activation**\n",
    "  * Multi-class: **n output neurons** with a **softmax activation**\n",
    "\n",
    "#### **Binary classification**\n",
    "\n",
    "```python\n",
    "output = torch.sigmoid(model(X))\n",
    "```\n",
    "\n",
    "→ Produces values in [0,1], interpretable as “probability of class 1”.\n",
    "\n",
    "#### **Multi-class classification**\n",
    "\n",
    "```python\n",
    "output = torch.softmax(model(X), dim=1)\n",
    "```\n",
    "\n",
    "→ Produces a probability distribution across classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "| Task                           | Typical Loss                                                  |\n",
    "| ------------------------------ | ------------------------------------------------------------- |\n",
    "| **Linear Regression**          | Mean Squared Error (`nn.MSELoss`)                             |\n",
    "| **Binary Classification**      | Binary Cross-Entropy (`nn.BCELoss` or `nn.BCEWithLogitsLoss`) |\n",
    "| **Multi-class Classification** | Cross-Entropy (`nn.CrossEntropyLoss`)                         |\n",
    "\n",
    "---\n",
    "\n",
    "## Activation + Loss Connection\n",
    "\n",
    "| Output Activation | Corresponding Loss               | Comment                    |\n",
    "| ----------------- | -------------------------------- | -------------------------- |\n",
    "| **None**          | `MSELoss`                        | regression                 |\n",
    "| **Sigmoid**       | `BCELoss` or `BCEWithLogitsLoss` | binary classification      |\n",
    "| **Softmax**       | `CrossEntropyLoss`               | multi-class classification |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary in One Line\n",
    "\n",
    "| Task                       | Output Layer | Activation | Loss                        |\n",
    "| -------------------------- | ------------ | ---------- | --------------------------- |\n",
    "| Linear Regression          | 1 neuron     | None       | MSELoss                     |\n",
    "| Binary Classification      | 1 neuron     | Sigmoid    | BCELoss / BCEWithLogitsLoss |\n",
    "| Multi-class Classification | n neurons    | Softmax    | CrossEntropyLoss            |\n",
    "\n",
    "---\n",
    "\n",
    " **So yes — you’re correct:**\n",
    "The *main architectural difference* between regression and classification networks is **the output activation (and the corresponding loss function)**.\n",
    "\n",
    "But conceptually, they also differ in **what they predict and how the output is interpreted** — continuous vs probability.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can show a **side-by-side code example** of the same dataset trained once as a regression and once as a classification network — you’ll see the differences in the last layer and loss directly.\n",
    "\n",
    "Would you like that?\n"
   ],
   "id": "ad9c90d12185fb60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
