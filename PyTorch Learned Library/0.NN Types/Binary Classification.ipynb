{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The Core Difference: What You’re Predicting\n",
    "\n",
    "| Task                  | Target (y)        | Goal                  | Example                      |\n",
    "| --------------------- | ----------------- | --------------------- | ---------------------------- |\n",
    "| **Linear Regression** | Continuous value  | Predict *how much*    | Predict house price          |\n",
    "| **Classification**    | Categorical label | Predict *which class* | Is this email spam? (Yes/No) |\n",
    "\n",
    "So the difference starts with **what the target variable represents**.\n",
    "Everything else — activation, loss function, and interpretation — follows from that.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture Differences\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "* **Output:** Usually **1 neuron**, no activation (raw value).\n",
    "* **Example:**\n",
    "\n",
    "  ```python\n",
    "  output = model(X)  # shape [N, 1]\n",
    "  ```\n",
    "* **Why no activation?**\n",
    "  Because regression outputs are continuous — you want to allow any real number.\n",
    "\n",
    "---\n",
    "\n",
    "### **Classification**\n",
    "\n",
    "* **Output:** Depends on the number of classes.\n",
    "\n",
    "  * Binary: **1 output neuron** with a **sigmoid activation**\n",
    "  * Multi-class: **n output neurons** with a **softmax activation**\n",
    "\n",
    "#### **Binary classification**\n",
    "\n",
    "```python\n",
    "output = torch.sigmoid(model(X))\n",
    "```\n",
    "\n",
    "→ Produces values in [0,1], interpretable as “probability of class 1”.\n",
    "\n",
    "#### **Multi-class classification**\n",
    "\n",
    "```python\n",
    "output = torch.softmax(model(X), dim=1)\n",
    "```\n",
    "\n",
    "→ Produces a probability distribution across classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "| Task                           | Typical Loss                                                  |\n",
    "| ------------------------------ | ------------------------------------------------------------- |\n",
    "| **Linear Regression**          | Mean Squared Error (`nn.MSELoss`)                             |\n",
    "| **Binary Classification**      | Binary Cross-Entropy (`nn.BCELoss` or `nn.BCEWithLogitsLoss`) |\n",
    "| **Multi-class Classification** | Cross-Entropy (`nn.CrossEntropyLoss`)                         |\n",
    "\n",
    "---\n",
    "\n",
    "## Activation + Loss Connection\n",
    "\n",
    "| Output Activation | Corresponding Loss               | Comment                    |\n",
    "| ----------------- | -------------------------------- | -------------------------- |\n",
    "| **None**          | `MSELoss`                        | regression                 |\n",
    "| **Sigmoid**       | `BCELoss` or `BCEWithLogitsLoss` | binary classification      |\n",
    "| **Softmax**       | `CrossEntropyLoss`               | multi-class classification |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary in One Line\n",
    "\n",
    "| Task                       | Output Layer | Activation | Loss                        |\n",
    "| -------------------------- | ------------ | ---------- | --------------------------- |\n",
    "| Linear Regression          | 1 neuron     | None       | MSELoss                     |\n",
    "| Binary Classification      | 1 neuron     | Sigmoid    | BCELoss / BCEWithLogitsLoss |\n",
    "| Multi-class Classification | n neurons    | Softmax    | CrossEntropyLoss            |\n",
    "\n",
    "---\n",
    "\n",
    " **So yes — you’re correct:**\n",
    "The *main architectural difference* between regression and classification networks is **the output activation (and the corresponding loss function)**.\n",
    "\n",
    "But conceptually, they also differ in **what they predict and how the output is interpreted** — continuous vs probability.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can show a **side-by-side code example** of the same dataset trained once as a regression and once as a classification network — you’ll see the differences in the last layer and loss directly.\n",
    "\n",
    "Would you like that?\n"
   ],
   "id": "90bf5e9dff13e6af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Titanic_model_V1\n",
    "\n",
    "This NN is built for a Binary Classification, because we are using nn.BCEWithLogitsLoss → expects raw logits directly so we don't need to output a softmax"
   ],
   "id": "66d3fab4a414d3c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Titanic_model_V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(5,20)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(20,30)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(30,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer3(self.relu2(self.layer2(self.relu1(self.layer1(x)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
