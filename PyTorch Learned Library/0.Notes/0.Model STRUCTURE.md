# Training-loop snippet (canonical)

```python
for x_batch, y_batch in train_loader:
    optimizer.zero_grad()           # A. clear old gradients
    y_pred = model(x_batch)         # B. forward pass
    loss = loss_fn(y_pred, y_batch) # C. compute scalar loss
    loss.backward()                 # D. compute gradients (backprop)
    optimizer.step()                # E. update parameters
```

---

# A — `optimizer.zero_grad()` (clear old gradients)

**What it does**

* Sets `.grad` of each parameter tracked by the optimizer to zero (usually via `param.grad = None` or `param.grad.zero_()` depending on optimizer implementation).
* Prepares the model so gradients computed by the next `backward()` are *only* from the current mini-batch (unless you intentionally want accumulation).

**Why it’s necessary**

* PyTorch **accumulates** gradients: calling `loss.backward()` adds to `.grad` rather than replacing it.
* If you don’t clear gradients, gradients from previous batches keep accumulating and will produce incorrect parameter updates.

**Effects on other steps**

* Must be called *before* `loss.backward()` in standard training. If called after `backward()`, you erase the freshly computed gradients and `optimizer.step()` will see zeros.
* If you intentionally want to accumulate gradients across multiple mini-batches (for effective larger batch size), *don’t* call `zero_grad()` every mini-batch — call it only after the accumulation steps.

**Performance note**

* Newer PyTorch versions often set `.grad` to `None` rather than zeroing memory — avoids extra memory writes and can be faster. Call `optimizer.zero_grad()` (preferred) rather than manually zeroing every param.

---

# B — `y_pred = model(x_batch)` (forward pass)

**What it does**

* Runs the input through the network’s layers (calls `forward()`), produces outputs (logits, probabilities, etc).
* Builds a computation graph (autograd graph) that records operations needed to compute gradients later. This graph links outputs back to parameters that require gradients.

**What gets tracked**

* Any tensor created from operations involving parameters with `requires_grad=True` becomes part of the graph.
* Intermediate activations are stored (by autograd) so gradients can be computed during the backward pass. Those stored activations consume GPU/CPU memory until the backward pass frees them.

**Effects on other steps**

* The output `y_pred` is used by the loss function to produce a scalar `loss`. The autograd graph created here is what `loss.backward()` will traverse to compute gradients.
* Setting `model.eval()` vs `model.train()` affects behavior of layers like BatchNorm and Dropout during forward; that, in turn, affects loss and gradients.

**Memory/perf considerations**

* Autograd stores activations; larger models / larger batch sizes use more memory.
* Use `torch.no_grad()` or `.detach()` when doing inference/evaluation to avoid storing unnecessary grads.

---

# C — `loss = loss_fn(y_pred, y_batch)` (compute scalar loss)

**What it does**

* Computes a scalar (or reduction to scalar, e.g., mean) that quantifies discrepancy between predictions and targets.
* Typical losses: `nn.CrossEntropyLoss`, `nn.MSELoss`, etc. Many of these return a scalar (required for `.backward()` without arguments).

**Why a scalar**

* `loss.backward()` computes gradients of this scalar w.r.t all leaf tensors with `requires_grad=True`.
* If your loss is not scalar (e.g., per-sample losses), you usually call `.sum()` or `.mean()` or call `loss.backward(gradient=...)` to provide proper upstream gradient.

**Effects on other steps**

* The numerical magnitude of the loss influences the gradient magnitudes; huge loss values → large gradients → possible unstable updates. This is one reason for normalization (mean loss) or gradient clipping.

**Common gotchas**

* Using incorrect reduction (sum vs mean) changes effective learning rate scale. If you switch from mean to sum, gradients scale with batch size → you may need to adjust learning rate accordingly.

---

# D — `loss.backward()` (autograd/backprop)

**What it does**

* Traverses the computation graph created during forward pass and computes gradients of `loss` with respect to parameters (`param.grad`) using reverse-mode automatic differentiation.
* For each parameter, autograd computes and *adds* the gradient to `param.grad` (i.e., accumulates).

**Internals**

* For each op in the graph, autograd calls the op’s `backward` function to compute local gradients and propagate them to upstream tensors.
* For leaf parameters (`nn.Parameter`), gradients accumulate in `.grad`.
* `loss.backward()` typically frees intermediate buffers once used, except if you set `retain_graph=True`.

**Options & special cases**

* `loss.backward(retain_graph=True)` keeps the computation graph in memory after backward. Useful if you need to call backward multiple times on same graph (rare in typical training).
* `loss.backward()` accepts a `gradient` argument when loss is not scalar (e.g., vector loss) — you must supply upstream gradient.
* You can call `.backward()` multiple times (e.g., for gradient accumulation across multiple mini-batches or multi-loss setups). Each call adds to `.grad`.

**Effects on other steps**

* Produces the gradients that `optimizer.step()` will read to update the parameters.
* If you zeroed gradients before backward, `.grad` will be exactly the gradients computed for the current batch (unless accumulating intentionally).
* If gradients are huge, `optimizer.step()` might make big jumps — you may use gradient clipping before step.

**Memory**

* Backward needs the graph and activations from forward, so they must remain in memory until backward finishes, unless using special techniques (checkpointing).

---

# E — `optimizer.step()` (parameter update)

**What it does**

* Uses each parameter’s `.grad` and optimizer internal state (momentum buffers, RMSprop running squares, Adam moving averages, etc.) to update parameter values.
* For `SGD` with momentum: uses momentum buffer + current gradient to compute parameter change.
* For `Adam`: updates biased first/second moment estimates then updates parameters with corrected moments.

**Important**

* `optimizer.step()` **reads** `.grad` values. If `.grad` is `None` or zero, the optimizer may do nothing for that parameter.
* If you accidentally call `optimizer.zero_grad()` after `loss.backward()` and before `step()`, `.grad` will be cleared and step will do nothing.

**Effects on other steps**

* After `step()`, parameters change. Next iteration’s forward pass uses updated parameters and a new computation graph will be built.
* Optimizer state (e.g., momentum buffers) persists across iterations and influences updates — this is why optimizer state must be saved for checkpointing if you want to resume training.

---

# Interactions & cause-effect chain (summary)

1. **Forward** builds graph and produces outputs; activations stored for backward.
2. **Loss** reduces outputs to scalar; magnitude scales gradients.
3. **`zero_grad()` before backward** ensures `.grad` values contain only current batch gradients.
4. **Backward** computes gradients and writes them into `.grad` (accumulates).
5. **Optionally clip gradients** (e.g., `torch.nn.utils.clip_grad_norm_`) to avoid exploding gradients.
6. **Optimizer step** updates parameters using `.grad` and optimizer internals (momentum, etc).
7. **Repeat** — new forward uses updated parameters.

---

# Extended topics — why details matter

### Gradient accumulation (deliberate)

* To simulate larger batch sizes when GPU memory is limited, do:

  ```python
  optimizer.zero_grad()
  for i, (x,y) in enumerate(small_batches):
      loss = model(x).loss_fn(y) / accumulation_steps
      loss.backward()
      if (i+1) % accumulation_steps == 0:
          optimizer.step()
          optimizer.zero_grad()
  ```
* Note: divide loss by accumulation_steps so gradient scale matches true large batch.

### `optimizer.zero_grad()` vs `model.zero_grad()`

* `optimizer.zero_grad()` only zeros grads for params the optimizer manages (preferred).
* `model.zero_grad()` zeros grads for all parameters in the model.
* Using `optimizer.zero_grad()` is common and efficient.

### Momentum, Adam, and learning rate interactions

* Optimizer maintains state (e.g., momentum buffer). Large gradients affect that state; zeroing gradients doesn’t touch optimizer state.
* Changing loss reduction (sum/mean) or batch size without adjusting learning rate can change optimization dynamics because gradients’ scale changes.

### Gradient clipping

* After `loss.backward()` but before `optimizer.step()`, you can clip gradients:

  ```python
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
  optimizer.step()
  ```
* Useful to prevent exploding gradients in RNNs / unstable training.

### `requires_grad`, `torch.no_grad()`, and `.detach()`

* Only tensors with `requires_grad=True` will accumulate gradients.
* Use `torch.no_grad()` during evaluation to avoid building graph and save memory.
* Use `.detach()` to use values in forward without including them in gradient computation.

### `retain_graph=True`

* Normally autograd frees the graph after backward. If you need to do multiple backward calls through same graph, set `retain_graph=True` (costs memory).

---

# Common mistakes & how to debug

* **Calling `zero_grad()` after `backward()`** — results in zeroed gradients on `step()`. Symptom: loss doesn’t decrease or parameters don’t change.
* **Mixing `sum` vs `mean` reduction** — effective learning rate changes; training becomes unstable if not adjusted.
* **Forgetting `.train()` / `.eval()`** — BatchNorm/Dropout behave differently; training in eval or vice versa will make training weird.
* **Large learning rates with large gradients** — cause divergence. Reduce LR or use gradient clipping.
* **GPU/CPU mismatch** — ensure `x_batch`, model, and targets are on the same device or you’ll get runtime errors.

---

# Minimal annotated example

```python
for x_batch, y_batch in train_loader:
    x_batch, y_batch = x_batch.to(device), y_batch.to(device)

    optimizer.zero_grad()            # 1) Clear old grads (prepare)
    y_pred = model(x_batch)          # 2) Forward: build graph, store activations
    loss = loss_fn(y_pred, y_batch)  # 3) Scalar loss computed from outputs
    loss.backward()                  # 4) Backprop: compute & accumulate grads
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional
    optimizer.step()                 # 5) Update params using computed grads
```

---

# Quick checklist when training

* [ ] Are you zeroing gradients before backward? (yes)
* [ ] Is loss scalar (or handled correctly)? (yes)
* [ ] Is model in `train()` mode? (for training)
* [ ] Are tensors and model on same device? (yes)
* [ ] Do you need gradient accumulation? (plan accordingly)
* [ ] Do you need gradient clipping? (check for exploding grads)
