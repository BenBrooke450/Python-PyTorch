

# What are *logits*?

* **Logits** are the **raw, unnormalized outputs** of a model **before applying an activation function** like `sigmoid` or `softmax`.
* In binary classification, the model usually outputs a single real number (any value from −∞ to +∞).
* That value is called the **logit**.

You can think of it like this:

```
input → Linear layer (Wx + b) → logit (z) → sigmoid(z) → probability (ŷ)
```

* The **logit** is the direct output of the linear layer (Wx + b).
* The **probability** is obtained after applying sigmoid:
  ŷ = 1 / (1 + exp(−z))

---

# Logits in `BCELoss`

* **`BCELoss`** expects you to feed **probabilities** (values between 0 and 1).
* That means **you must apply sigmoid** to your logits **before** passing them in:

```python
logits = model(x)               # raw outputs (logits)
y_pred = torch.sigmoid(logits)  # convert to probability
loss = nn.BCELoss()(y_pred, y_true)
```

---

# Why is this a problem?

* If you use `BCELoss` directly with logits (without sigmoid), the loss won’t make sense — since BCE expects inputs between 0 and 1.
* Example: a logit = 5 → sigmoid(5) ≈ 0.99 (valid probability), but if you pass `5` directly, `BCELoss` thinks that’s already a probability, which breaks the math.

---

# How `BCEWithLogitsLoss` fixes it

* `BCEWithLogitsLoss` combines **sigmoid + BCE** in one function.
* It takes logits directly and internally applies the sigmoid in a **numerically stable way**.
* This avoids issues like `log(0)` when probabilities are very close to 0 or 1.

```python
logits = model(x)                       # raw outputs (logits)
loss = nn.BCEWithLogitsLoss()(logits, y_true)  # no need for sigmoid
```

---

# Summary

* **Logits = raw outputs (−∞ to +∞) before sigmoid**.
* **BCELoss expects probabilities** → you must manually apply `sigmoid(logits)`.
* **BCEWithLogitsLoss expects logits** → applies sigmoid inside, safer and preferred.

