

## ğŸš€ What is Mini-Batch Gradient Descent?

Itâ€™s a **variant of gradient descent** where instead of:

* **Batch Gradient Descent** â†’ using the *entire dataset* each update (slow, memory heavy).
* **Stochastic Gradient Descent (SGD)** â†’ using *one sample at a time* (fast but noisy).

**Mini-Batch Gradient Descent** finds the sweet spot:
ğŸ‘‰ Split dataset into **small batches** (e.g., 32, 64, 128 samples).
ğŸ‘‰ For each batch, compute gradient & update weights.

This balances **efficiency** and **stability**.

---

## âš™ï¸ Algorithm Steps

Given:

* Dataset of size $N$
* Batch size $m$ (say, 32)
* Learning rate $\eta$

1. Shuffle dataset.
2. Split into batches: $\{B_1, B_2, ..., B_k\}$, where $k = N/m$.
3. For each batch $B_j$:

   * Forward pass â†’ compute predictions.
   * Compute **loss** on that batch.
   * Backpropagate â†’ compute gradients wrt weights.
   * Update weights:

   $$
   \theta := \theta - \eta \cdot \nabla_\theta J(\theta; B_j)
   $$

   where $J(\theta; B_j)$ is the cost on batch $B_j$.

---

## ğŸ§® Math Behind It

* Cost function for whole dataset:

  $$
  J(\theta) = \frac{1}{N} \sum_{i=1}^N L(f_\theta(x_i), y_i)
  $$

* Mini-batch approximation with batch $B_j$:

  $$
  J_{B_j}(\theta) = \frac{1}{m} \sum_{i \in B_j} L(f_\theta(x_i), y_i)
  $$

* Gradient update:

  $$
  \theta := \theta - \eta \cdot \nabla_\theta J_{B_j}(\theta)
  $$

So MBGD is like an **unbiased estimator** of the full gradient.

---

## âœ… Advantages

* Faster than full-batch.
* Less noisy than pure SGD.
* Works well with GPUs (parallelization).
* Introduces some noise â†’ helps escape local minima.

---

## ğŸ”¹ Example in PyTorch

```python
import torch
from torch.utils.data import DataLoader, TensorDataset

# Example dataset
X = torch.randn(100, 10)
y = torch.randint(0, 2, (100,))

dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for X_batch, y_batch in dataloader:
    # Forward pass, compute loss
    y_pred = model(X_batch)
    loss = criterion(y_pred, y_batch)

    # Backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

