

## **1. Loss Function**

### **What it is**

* The **loss function** (a.k.a. cost function) measures **how wrong** the network’s predictions are compared to the true labels.
* It’s a **single number** that the optimizer tries to minimize.
* Smaller loss → better predictions.

### **Common loss functions**

* **MSE (Mean Squared Error):** for regression
* **CrossEntropyLoss:** for classification
* **BCELoss (Binary Cross-Entropy):** for binary classification

---

### **Example (Loss Function in PyTorch)**

```python
import torch
import torch.nn as nn

# Suppose predicted outputs (logits) and true labels
pred = torch.tensor([[0.2, 0.8]], requires_grad=True)  # model prediction
target = torch.tensor([1])  # true label

criterion = nn.CrossEntropyLoss()   # loss function
loss = criterion(pred, target)

print("Loss:", loss.item())
```

Here:

* `pred` = model’s guess
* `target` = actual label
* `loss` = how far off the guess is

---

<br><br><br>


## **2. Optimizer**

### **What it is**

* The **optimizer** updates the model’s weights based on the gradients from backpropagation.
* It decides **how to adjust parameters** to reduce the loss.

### **Common optimizers**

* **SGD (Stochastic Gradient Descent):** simple and widely used
* **Adam:** adaptive, usually faster convergence
* **RMSprop:** useful for recurrent networks

---

### **Example (Optimizer in PyTorch)**

```python
import torch.optim as optim

# Suppose we have a simple linear model
model = nn.Linear(2, 1)

# Use SGD optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Forward pass
inputs = torch.tensor([[1.0, 2.0]])
target = torch.tensor([[1.0]])
output = model(inputs)

# Compute loss
criterion = nn.MSELoss()
loss = criterion(output, target)

# Backpropagation + weight update
optimizer.zero_grad()   # reset gradients
loss.backward()         # compute gradients
optimizer.step()        # update weights
```

---

## ✅ **Summary**

* **Loss function:** Measures *how wrong* the model is. (e.g., `nn.CrossEntropyLoss`, `nn.MSELoss`)
* **Optimizer:** Adjusts the weights to minimize the loss. (e.g., `optim.SGD`, `optim.Adam`)
* Together:

  1. Forward pass → compute prediction
  2. Compute loss → how wrong?
  3. Backward pass → gradients
  4. Optimizer step → update weights

---

