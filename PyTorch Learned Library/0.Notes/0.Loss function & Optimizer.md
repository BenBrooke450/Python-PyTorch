

## **1. Loss Function**

### **What it is**

* The **loss function** (a.k.a. cost function) measures **how wrong** the network‚Äôs predictions are compared to the true labels.
* It‚Äôs a **single number** that the optimizer tries to minimize.
* Smaller loss ‚Üí better predictions.

### **Common loss functions**

* **MSE (Mean Squared Error):** for regression
* **CrossEntropyLoss:** for classification
* **BCELoss (Binary Cross-Entropy):** for binary classification

---

### **Example (Loss Function in PyTorch)**

```python
import torch
import torch.nn as nn

# Suppose predicted outputs (logits) and true labels
pred = torch.tensor([[0.2, 0.8]], requires_grad=True)  # model prediction
target = torch.tensor([1])  # true label

criterion = nn.CrossEntropyLoss()   # loss function
loss = criterion(pred, target)

print("Loss:", loss.item())
```

Here:

* `pred` = model‚Äôs guess
* `target` = actual label
* `loss` = how far off the guess is

---

<br><br><br>


## **2. Optimizer**

### **What it is**

* The **optimizer** updates the model‚Äôs weights based on the gradients from backpropagation.
* It decides **how to adjust parameters** to reduce the loss.

### **Common optimizers**

* **SGD (Stochastic Gradient Descent):** simple and widely used
* **Adam:** adaptive, usually faster convergence
* **RMSprop:** useful for recurrent networks

---

### **Example (Optimizer in PyTorch)**

```python
import torch.optim as optim

# Suppose we have a simple linear model
model = nn.Linear(2, 1)

# Use SGD optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Forward pass
inputs = torch.tensor([[1.0, 2.0]])
target = torch.tensor([[1.0]])
output = model(inputs)

# Compute loss
criterion = nn.MSELoss()
loss = criterion(output, target)

# Backpropagation + weight update
optimizer.zero_grad()   # reset gradients
loss.backward()         # compute gradients
optimizer.step()        # update weights
```

---

## ‚úÖ **Summary**

* **Loss function:** Measures *how wrong* the model is. (e.g., `nn.CrossEntropyLoss`, `nn.MSELoss`)
* **Optimizer:** Adjusts the weights to minimize the loss. (e.g., `optim.SGD`, `optim.Adam`)
* Together:

  1. Forward pass ‚Üí compute prediction
  2. Compute loss ‚Üí how wrong?
  3. Backward pass ‚Üí gradients
  4. Optimizer step ‚Üí update weights

---


Perfect ‚Äî let‚Äôs carefully rebuild the table so it‚Äôs **clean, correct, and readable in plain text** (no strange LaTeX characters). I‚Äôll also group the losses by **problem type** so you can study them faster.

---

# üìä Common Loss Functions in Machine Learning

---

## üîµ Regression Losses

| **Loss Function**                  | **Formula**                                | **Usage**                                          |                                |                                               |         |                                             |
| ---------------------------------- |--------------------------------------------| -------------------------------------------------- | ------------------------------ | --------------------------------------------- | ------- | ------------------------------------------- |
| **Mean Squared Error (MSE)**       | L = (1/n) ¬∑ Œ£ (y·µ¢ ‚àí ≈∑·µ¢)¬≤                   | Standard regression (linear regression, neural nets) |                                |                                               |         |                                             |
| **Mean Absolute Error (MAE / L1)** | L = (1/n) ¬∑ Œ£ y·µ¢ ‚àí ≈∑·µ¢                      | Robust regression, less sensitive to outliers |         |                                             |
| **Huber Loss**                     | If  y·µ¢ ‚àí ≈∑·µ¢ ‚â§ Œ¥ ‚Üí 0.5¬∑(y·µ¢ ‚àí ≈∑·µ¢)¬≤, else Œ¥¬∑(y·µ¢ ‚àí ≈∑·µ¢‚àí 0.5Œ¥) | Regression with outliers (mix of MSE & MAE) |
| **Poisson Loss**                   | L = (1/n) ¬∑ Œ£ (≈∑·µ¢ ‚àí y·µ¢¬∑log(≈∑·µ¢))            | Count regression (events, arrivals, frequencies)   |                                |                                               |         |                                             |

---

## üü¢ Classification Losses

| **Loss Function**                   | **Formula**                                    | **Usage**                                                      |                                                                   |
| ----------------------------------- |------------------------------------------------| -------------------------------------------------------------- | ----------------------------------------------------------------- |
| **Binary Cross-Entropy (BCE)**      | L = ‚àí(1/n) ¬∑ Œ£ \[y·µ¢¬∑log(≈∑·µ¢) + (1‚àíy·µ¢)¬∑log(1‚àí≈∑·µ¢)] | Binary classification (logistic regression, anomaly detection) |                                                                   |
| **Categorical Cross-Entropy (CCE)** | L = ‚àí(1/n) ¬∑ Œ£ Œ£ y·µ¢,c ¬∑ log(≈∑·µ¢,c)              | Multi-class classification (image/NLP tasks)                   |                                                                   |
| **Negative Log-Likelihood (NLL)**   | L = ‚àí(1/n) ¬∑ Œ£ log p(y·µ¢,x·µ¢)                    | Classification with probabilistic models (used with `LogSoftmax`) |
| **Hinge Loss**                      | L = (1/n) ¬∑ Œ£ max(0, 1 ‚àí y·µ¢¬∑≈∑·µ¢)                | Support Vector Machines (SVMs)                                 |                                                                   |
| **Squared Hinge Loss**              | L = (1/n) ¬∑ Œ£ \[max(0, 1 ‚àí y·µ¢¬∑≈∑·µ¢)]¬≤            | SVMs (smooth gradient version)                                 |                                                                   |

---

## üü£ Similarity / Metric Learning

| **Loss Function**         | **Formula**                     | **Usage**                                                          |
| ------------------------- | ------------------------------- | ------------------------------------------------------------------ |
| **Cosine Embedding Loss** | L = 1 ‚àí (≈∑ ¬∑ y) / (‚Äñ≈∑‚Äñ ¬∑ ‚Äñy‚Äñ)   | Sentence embeddings, recommender systems                           |
| **Triplet Loss**          | L = max(0, d(a,p) ‚àí d(a,n) + Œ±) | Face recognition, metric learning (anchor-positive-negative setup) |

---

## üî¥ Generative / Sequence Models

| **Loss Function**                                    | **Formula**                                              | **Usage**                                             |
| ---------------------------------------------------- | -------------------------------------------------------- | ----------------------------------------------------- |
| **Kullback‚ÄìLeibler (KL) Divergence**                 | D‚Çñ‚Çó(P‚ÄñQ) = Œ£ P(x) ¬∑ log(P(x)/Q(x))                       | Variational autoencoders (VAEs), Bayesian ML          |
| **CTC Loss (Connectionist Temporal Classification)** | Computed via dynamic programming (no simple closed form) | Speech recognition, OCR, sequence alignment           |
| **Wasserstein Loss**                                 | W(P,Q) = inf over couplings Œ≥ E\[‚Äñx‚àíy‚Äñ]                  | Wasserstein GANs (stable GAN training)                |
| **Adversarial Loss (GAN)**                           | min\_G max\_D \[E(log D(x)) + E(log(1 ‚àí D(G(z))))]       | Generative Adversarial Networks (images, text, audio) |

---

‚úÖ This version is **structured by problem type** and all formulas are in **plain text** so they‚Äôll look normal in any editor.

Do you want me to also create a **one-page PDF ‚Äúcheat sheet‚Äù** of this table, so you can keep it handy while studying or coding?

