

### **1. Purpose of Activation Functions**

Activation functions introduce **non-linearity** into the network. Without them, no matter how many layers you stack, the network would behave like a single linear transformation — essentially just a linear model.

Mathematically:

$$
y = W_3(W_2(W_1 x)) = W_{\text{combined}} x
$$

This is still just linear. Non-linear activations (like ReLU, sigmoid, tanh) allow the network to approximate complex functions.

---

### **2. Why Multiple Activations Between Hidden Layers**

There are a few reasons people sometimes use different activations in different layers:

#### **a) Layer-specific behavior**

* Some activations work better in early layers (e.g., **ReLU** for sparsity and avoiding vanishing gradients).
* Others might help in later layers (e.g., **tanh** for centered outputs).

#### **b) Encourage richer representations**

* Different activations can create **diverse transformations**.
* For example, a layer with **ReLU** followed by a layer with **sigmoid** can capture different types of non-linear patterns.

#### **c) Experimental / empirical tuning**

* Some architectures, especially in deep learning research, found that mixing activations improves training stability or final performance.
* For example, using **GELU** in transformers but **ReLU** elsewhere.

---

### **3. Practical Example**

Imagine a 3-layer network:

```
Input → Dense → ReLU → Dense → tanh → Dense → Output
```

* **ReLU** in the first hidden layer: Helps model sparse features and avoids vanishing gradients.
* **tanh** in the second hidden layer: Compresses outputs into \[-1,1], which might stabilize learning or improve feature representation.

---

### **4. Things to Watch Out For**

* Using **too many different activations** can make training unstable.
* Often, the simplest approach works: **same activation (ReLU)** for all hidden layers, and a special one (sigmoid, softmax, etc.) at the output depending on the task.

---

✅ **Summary:**
Multiple activations between hidden layers are used to **introduce different types of non-linear transformations**, encourage richer feature representations, and sometimes improve training dynamics. But it’s not mandatory — most standard networks just use **one activation type throughout hidden layers**.





