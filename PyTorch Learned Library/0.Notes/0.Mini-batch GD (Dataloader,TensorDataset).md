

## What is Mini-Batch Gradient Descent?

It’s a **variant of gradient descent** where instead of:

* **Batch Gradient Descent** → using the *entire dataset* each update (slow, memory heavy).
* **Stochastic Gradient Descent (SGD)** → using *one sample at a time* (fast but noisy).

**Mini-Batch Gradient Descent** finds the sweet spot:
Split dataset into **small batches** (e.g., 32, 64, 128 samples).
For each batch, compute gradient & update weights.

This balances **efficiency** and **stability**.

---

## Algorithm Steps

Given:

* Dataset of size $N$
* Batch size $m$ (say, 32)
* Learning rate $\eta$

1. Shuffle dataset.
2. Split into batches: $\{B_1, B_2, ..., B_k\}$, where $k = N/m$.
3. For each batch $B_j$:

   * Forward pass → compute predictions.
   * Compute **loss** on that batch.
   * Backpropagate → compute gradients wrt weights.
   * Update weights:

   $$
   \theta := \theta - \eta \cdot \nabla_\theta J(\theta; B_j)
   $$

   where $J(\theta; B_j)$ is the cost on batch $B_j$.

---

## Math Behind It

* Cost function for whole dataset:

  $$
  J(\theta) = \frac{1}{N} \sum_{i=1}^N L(f_\theta(x_i), y_i)
  $$

* Mini-batch approximation with batch $B_j$:

  $$
  J_{B_j}(\theta) = \frac{1}{m} \sum_{i \in B_j} L(f_\theta(x_i), y_i)
  $$

* Gradient update:

  $$
  \theta := \theta - \eta \cdot \nabla_\theta J_{B_j}(\theta)
  $$

So MBGD is like an **unbiased estimator** of the full gradient.

---

## Advantages

* Faster than full-batch.
* Less noisy than pure SGD.
* Works well with GPUs (parallelization).
* Introduces some noise → helps escape local minima.

---

## Example in PyTorch

```python
import torch
from torch.utils.data import DataLoader, TensorDataset

# Example dataset
X = torch.randn(100, 10)
y = torch.randint(0, 2, (100,))

dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for X_batch, y_batch in dataloader:
    # Forward pass, compute loss
    y_pred = model(X_batch)
    loss = criterion(y_pred, y_batch)

    # Backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```



<br><br><br><br><br><br><br><br><br>





## 1️⃣ Using `DataLoader` with batch size 1

```python
from torch.utils.data import TensorDataset, DataLoader

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# Create DataLoaders with batch_size = 1
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
```

* `shuffle=True` → ensures a different random order each epoch
* `batch_size=1` → each iteration feeds the model **a single sample**

---

## 2️⃣ Training Loop One Sample at a Time

```python
epochs = 50

for epoch in range(epochs):
    model_iris_one.train()
    for X_sample, y_sample in train_loader:    # each X_sample is 1x4
        optimizer.zero_grad()
        y_pred_logits = model_iris_one(X_sample)
        loss = loss_fn(y_pred_logits, y_sample.long())
        loss.backward()
        optimizer.step()

    # Evaluation
    model_iris_one.eval()
    with torch.inference_mode():
        correct = 0
        total = 0
        for X_sample, y_sample in test_loader:
            y_pred_logits = model_iris_one(X_sample)
            y_pred_class = torch.argmax(y_pred_logits, dim=1)
            correct += (y_pred_class == y_sample).sum().item()
            total += y_sample.size(0)

        test_acc = correct / total
        print(f"Epoch {epoch} | Test Accuracy: {test_acc*100:.2f}%")
```

---

## 3️⃣ Notes

1. **Loss computation**: Still works normally — just computed on one sample at a time.
2. **Accuracy**: Accumulate correct predictions over all samples.
3. **Performance**: Feeding one sample at a time is slower, especially on GPU, but fine for **small datasets like Iris**.
4. **Learning behavior**:

   * This is **true stochastic gradient descent**.
   * Weights are updated after each sample → more noisy but can converge faster for some problems.

---

### ⚡ Alternative

If you don’t want to use `DataLoader`, you can also just loop manually:

```python
for i in range(len(X_train)):
    x = X_train[i].unsqueeze(0)  # shape [1, 4]
    y = y_train[i].unsqueeze(0)  # shape [1]
    optimizer.zero_grad()
    y_pred = model_iris_one(x)
    loss = loss_fn(y_pred, y.long())
    loss.backward()
    optimizer.step()
```

* `.unsqueeze(0)` is needed because `nn.Linear` expects **2D input** `[batch_size, features]`.












