# **Stochastic Gradient Descent (SGD)** 

---

## What is SGD?

* **Gradient Descent** is an optimization method to minimize a cost function $J(\theta)$.
* Instead of computing gradients on the *entire dataset* (Batch GD), **SGD** updates parameters using **only one training example at a time**.

So each update is based on a **single sample**:

$$
\theta := \theta - \eta \cdot \nabla_\theta J(\theta; x_i, y_i)
$$

---

## How It Works

1. Shuffle the dataset.
2. For each training example $(x_i, y_i)$:

   * Predict output.
   * Compute loss $L(f_\theta(x_i), y_i)$.
   * Compute gradient wrt weights.
   * Update weights immediately.

---

## Math Behind It

* Full Batch Gradient Descent gradient:

  $$
  \nabla_\theta J(\theta) = \frac{1}{N} \sum_{i=1}^N \nabla_\theta L(f_\theta(x_i), y_i)
  $$

* Stochastic Gradient Descent gradient approximation (single sample):

  $$
  \nabla_\theta J(\theta) \approx \nabla_\theta L(f_\theta(x_i), y_i)
  $$

* Update rule:

  $$
  \theta := \theta - \eta \cdot \nabla_\theta L(f_\theta(x_i), y_i)
  $$

---

## Advantages

* Very **fast** updates (one sample).
* Adds **randomness/noise** â†’ helps escape local minima or saddle points.
* Good for **large datasets** (no need to load all data).

---

## Disadvantages

* Very **noisy path** toward convergence.
* Harder to choose good learning rate.
* May **oscillate** around the minimum instead of converging smoothly.

---

## Example in PyTorch

```python
import torch
import torch.optim as optim

model = torch.nn.Linear(10, 1)
criterion = torch.nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

for epoch in range(10):
    for i in range(len(X)):   # one sample at a time
        x_sample = X[i].unsqueeze(0)  # shape [1, features]
        y_sample = y[i].unsqueeze(0)

        y_pred = model(x_sample)
        loss = criterion(y_pred, y_sample)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## ðŸš¦ Summary

* **Batch GD**: Uses all samples â†’ stable but slow.
* **SGD**: Uses one sample â†’ fast, noisy, can escape local minima.
* **Mini-Batch GD**: Compromise â†’ small batches (most used in practice).

