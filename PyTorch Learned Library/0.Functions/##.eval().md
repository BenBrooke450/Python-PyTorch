

# üîπ `model.train()` vs `model.eval()`

In PyTorch, every `nn.Module` (your model) can be in **training mode** or **evaluation mode**.
You switch between them using:

```python
model.train()   # set to training mode
model.eval()    # set to evaluation (inference) mode
```

---

## üî∏ `model.train()`

* **Purpose**: Tell PyTorch that the model is in *training mode*.

* **What it does:**

  1. Enables behaviors specific to training:

     * **Dropout layers (`nn.Dropout`)**: will randomly zero out some neuron outputs to prevent overfitting.

       * Example: with `p=0.5`, each neuron has a 50% chance of being zeroed out during training.
     * **Batch Normalization (`nn.BatchNorm1d`, `nn.BatchNorm2d`, etc.)**: will use **mini-batch statistics** (mean and variance) to normalize activations, and update its running averages.
  2. Other layers remain unaffected (e.g., Linear, Conv).

* **When to use:**
  Always call `model.train()` before running your training loop.

---

## üî∏ `model.eval()`

* **Purpose**: Tell PyTorch that the model is in *evaluation/inference mode*.

* **What it does:**

  1. Disables training-specific behaviors:

     * **Dropout**: no random zeroing ‚Äî instead, neurons are scaled deterministically (outputs are multiplied by keep probability, e.g. 0.5 ‚Üí scale by 0.5).
     * **BatchNorm**: uses the **running averages** of mean/variance collected during training, instead of recalculating from the current batch.
  2. Makes model predictions **deterministic** and consistent.

* **When to use:**
  Always call `model.eval()` during validation/testing/inference.

---

## üî∏ ‚ö†Ô∏è Common Mistakes

1. Forgetting to switch to `model.eval()` during evaluation:

   * Your validation/test accuracy will look noisy or lower than expected because Dropout and BatchNorm are still behaving like training.

2. Forgetting to switch back to `model.train()` before training continues:

   * Dropout won‚Äôt drop neurons and BatchNorm won‚Äôt update running averages ‚Üí model might not learn properly.

3. Confusion:

   * `model.eval()` **does not stop gradient tracking**. You should still wrap evaluation in `torch.no_grad()` to save memory and speed up inference.

---

## üî∏ Typical Usage Pattern

```python
for epoch in range(num_epochs):
    # ---- Training ----
    model.train()  # enable dropout, batchnorm updates
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

    # ---- Validation ----
    model.eval()   # disable dropout, batchnorm updates
    val_loss = 0
    with torch.no_grad():  # disable autograd for inference
        for inputs, targets in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
    print(f"Epoch {epoch}, Validation loss: {val_loss/len(val_loader)}")
```

---

## üî∏ Summary Table

| Mode       | Dropout                        | BatchNorm                                            | Gradients?                           | Typical Usage                  |
| ---------- | ------------------------------ | ---------------------------------------------------- | ------------------------------------ | ------------------------------ |
| `.train()` | Active (randomly zero neurons) | Uses **batch statistics** (updates running mean/var) | Yes (autograd builds graph)          | Training loop                  |
| `.eval()`  | Disabled (no dropout)          | Uses **running statistics** (frozen mean/var)        | No (if wrapped in `torch.no_grad()`) | Validation, testing, inference |

---

‚úÖ **Key takeaway:**

* `.train()` and `.eval()` don‚Äôt directly affect gradients or optimizers.
* They just change **layer behaviors** (Dropout & BatchNorm).
* Use `.train()` for training, `.eval()` + `torch.no_grad()` for validation/testing.

---

Would you like me to also **draw a diagram** showing the difference in data flow between `.train()` and `.eval()` with Dropout and BatchNorm?
